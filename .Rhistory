install.packages(c("DataExplorer", "glmnet", "moments", "pdp", "rBayesianOptimization", "vita", "yardstick"))
rm(list = ls())
library(readr)
#set the working directory
setwd("E:\MSK\Data Science\Edwisor data science\Project\Santander")
#set the working directory
setwd("E:/MSK/Data Science\Edwisor data science\Project\Santander")
#set the working directory
setwd("E:/MSK/Data Science/Edwisor data science/Project/Santander")
#Check the current working directory
getwd()
train <- read.csv("train.csv")
#viewing the train data set
View(train)
#Str() displays the contents of the data of each variable,their datatypes
str(train)
dim(train)
dim(test_df)
#Reading the given CSV files
test_df <- read.csv("test.csv")
dim(train)
dim(test_df)
#displays the first 5 rows of the data
head(train)
#Creating a data frame of missing values of both train and test dataset
missing_val_train = data.frame(apply(train,2,function(x){sum(is.na(x))}))
missing_val_test = data.frame(apply(test_df,2,function(x){sum(is.na(x))}))
#Naming the missing value data set
names(missing_val_train)[1] = "Missing_Values_train"
names(missing_val_test)[1] = "Missing_Values_test"
#Calculating the percentage of missing values
missing_per_train = (missing_val_train$Missing_Values_train/nrow(train))*100
missing_per_test = (missing_val_test$Missing_Values_test/nrow(test_df))*100
#We observe that there are no missing values in both the train and test data set
#convert the target var of train data to factor
train$target<-as.factor(train$target)
#load and attach add-on packages
require(gridExtra)
#Count of target classes
table(train$target)
#Percenatge counts of target classes
table(train$target)/length(train$target)*100
#Bar plot for count of target classes
library(ggplot2)
ggplot(train,aes(target))+theme_bw()+geom_bar(stat='count',fill='pink')
#We have a unbalanced data,where 90% of the data is the number of customers those will not make a transaction and 10% of the data is those who will make a transaction.
#Distribution of train attributes from 3 to 102
for (var in names(train)[c(3:102)]){
target<-train$target
plot<-ggplot(train, aes(x=train[[var]],fill=target)) +
geom_density(kernel='gaussian') + ggtitle(var)+theme_classic()
print(plot)
}
#Distribution of train attributes from 103 to 202
for (var in names(train)[c(103:202)]){
target<-train$target
plot<-ggplot(train, aes(x=train[[var]], fill=target)) +
geom_density(kernel='gaussian') + ggtitle(var)+theme_classic()
print(plot)
}
#We observed that there are considerable number of features which significantly have different distributions for two target variables. For example like var_0,var_1,var_9,var_198 var_180 etc.
#We observed that there are considerable number of features which significantly have same distributions for two target variables. For example like var_3,var_7,var_10,var_171,var_185 etc.
#Find mean values per row in train and test data.
train_mean<-apply(train[,-c(1,2)],MARGIN=1,FUN=mean)
test_mean<-apply(test_df[,-c(1)],MARGIN=1,FUN=mean)
ggplot()+
#Distribution of mean values per row in train data
geom_density(data=train[,-c(1,2)],aes(x=train_mean),kernel='gaussian',show.legend=TRUE,color='black')+theme_classic()+
#Distribution of mean values per row in test data
geom_density(data=test_df[,-c(1)],aes(x=test_mean),kernel='gaussian',show.legend=TRUE,color='purple')+
labs(x='mean values per row',title="Distribution of mean values per row in train and test dataset")
#find mean values per column in train and test data.
train_mean<-apply(train[,-c(1,2)],MARGIN=2,FUN=mean)
test_mean<-apply(test_df[,-c(1)],MARGIN=2,FUN=mean)
ggplot()+
#Distribution of mean values per column in train data
geom_density(aes(x=train_mean),kernel='gaussian',show.legend=TRUE,color='red')+theme_classic()+
#Distribution of mean values per column in test data
geom_density(aes(x=test_mean),kernel='gaussian',show.legend=TRUE,color='yellow')+
labs(x='mean values per column',title="Distribution of mean values per column in train and test dataset")
#find standard deviation values per row in train and test data.
train_sd<-apply(train[,-c(1,2)],MARGIN=1,FUN=sd)
test_sd<-apply(test_df[,-c(1)],MARGIN=1,FUN=sd)
ggplot()+
#Distribution of sd values per row in train data
geom_density(data=train[,-c(1,2)],aes(x=train_sd),kernel='gaussian',show.legend=TRUE,color='red')+theme_classic()+
#Distribution of mean values per row in test data
geom_density(data=test_df[,-c(1)],aes(x=test_sd),kernel='gaussian',show.legend=TRUE,color='blue')+
labs(x='sd values per row',title="Distribution of sd values per row in train and test dataset")
#find sd values per column in train and test data.
train_sd<-apply(train[,-c(1,2)],MARGIN=2,FUN=sd)
test_sd<-apply(test_df[,-c(1)],MARGIN=2,FUN=sd)
ggplot()+
#Distribution of sd values per column in train data
geom_density(aes(x=train_sd),kernel='gaussian',show.legend=TRUE,color='red')+theme_classic()+
#Distribution of sd values per column in test data
geom_density(aes(x=test_sd),kernel='gaussian',show.legend=TRUE,color='yellow')+
labs(x='sd values per column',title="Distribution of sd values per column in train and test dataset")
ggplot()+
#Distribution of sd values per column in train data
geom_density(aes(x=train_sd),kernel='gaussian',show.legend=TRUE,color='red')+theme_classic()+
#Distribution of sd values per column in test data
geom_density(aes(x=test_sd),kernel='gaussian',show.legend=TRUE,color='green')+
labs(x='sd values per column',title="Distribution of sd values per column in train and test dataset")
ggplot()+
#Distribution of sd values per column in train data
geom_density(aes(x=train_sd),kernel='gaussian',show.legend=TRUE,color='purple')+theme_classic()+
#Distribution of sd values per column in test data
geom_density(aes(x=test_sd),kernel='gaussian',show.legend=TRUE,color='green')+
labs(x='sd values per column',title="Distribution of sd values per column in train and test dataset")
#Correlations in train data
#convert factor to int
train$target<-as.numeric(train$target)
train_correlations<-cor(train[,c(2:202)])
train_correlations
#Correlations in test data
test_correlations<-cor(test_df[,c(2:201)])
test_correlations
#We observed that the correlation between the test attributes is very small.
#Feature engineering
#Variable importance
#Variable importance is used to see top features in dataset based on mean decreses gini.
#Let us build simple model to find features which are more important.
#Split the training data using simple random sampling
train_index<-sample(1:nrow(train),0.75*nrow(train))
#train data
train_data<-train[train_index,]
#validation data
valid_data<-train[-train_index,]
#dimension of train and validation data
dim(train_data)
dim(valid_data)
# Random forest classifier
#Training the Random forest classifier
set.seed(2732)
#convert to int to factor
train_data$target<-as.factor(train_data$target)
#setting the mtry
mtry<-floor(sqrt(200))
#setting the tunegrid
tuneGrid<-expand.grid(.mtry=mtry)
#fitting the random forest
library(randomForest)
rf<-randomForest(target~.,train_data[,-c(1)],mtry=mtry,ntree=10,importance=TRUE)
#Feature importance by random forest
#Variable importance
VarImp<-importance(rf,type=2)
VarImp
#We can observe that the top important features are var_12, var_26, var_22,v var_174, var_198 and so on based on Mean decrease gini.
# Random forest classifier
#Training the Random forest classifier
set.seed(2732)
#convert to int to factor
train_data$target<-as.factor(train_data$target)
#setting the mtry
mtry<-floor(sqrt(200))
#setting the tunegrid
tuneGrid<-expand.grid(.mtry=mtry)
#fitting the random forest
library(randomForest)
rf<-randomForest(target~.,train_data[,-c(1)],mtry=mtry,ntree=10,importance=TRUE)
#Feature importance by random forest
#Variable importance
VarImp<-importance(rf,type=2)
VarImp
#We can observe that the top important features are var_12, var_26, var_22,v var_174, var_198 and so on based on Mean decrease gini.
# Random forest classifier
#Training the Random forest classifier
set.seed(2732)
#convert to int to factor
train_data$target<-as.factor(train_data$target)
#setting the mtry
mtry<-floor(sqrt(200))
#setting the tunegrid
tuneGrid<-expand.grid(.mtry=mtry)
#fitting the random forest
library(randomForest)
rf<-randomForest(target~.,train_data[,-c(1)],mtry=mtry,ntree=10,importance=TRUE)
